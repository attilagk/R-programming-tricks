---
layout: default
title: Classification
tags: [ classification ]
Rdir: "/R/2017-10-17-classification/"
featimg: "tree-1.png"
---

Decision tree model is fitted to toy data on homes in New York and San Francisco (NY, SF).  Along the way overfitting is illustrated.  The optimally fitted tree is used to classify some test data as either NY or SF.

Download the related presentation [here]({{ site.baseurl }}/assets/machine-learning-attilagk.pdf)

## Data and analytical tools

We are going to use the home data and the machine learning approach called decision tree or CART (Classification And Regression Tree) from [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), referred to here as *visual intro*.  As CART implementation we are going to take advantage of the `rpart` package.  As usual in this blog, we opt for `lattice` graphics.

```{r}
library(lattice)
library(rpart)
lattice.options(default.args = list(as.table = TRUE))
lattice.options(default.theme = "standard.theme")
opts_chunk$set(dpi = 144)
opts_chunk$set(out.width = "700px")
opts_chunk$set(dev = c("png", "pdf"))
```

Now read the data...

```{r}
home <- read.csv("ny-sf-home-data.csv")
home <- cbind(data.frame(city = factor(home$in_sf)), home)
levels(home$city) <- c("NY", "SF")
head(home, n = 2)
tail(home, n = 2)
```

So our data are `r nrow(home)` observations on homes in two cities: New York and San Francisco ($$\mathrm{NY},\mathrm{SF}$$).  We'll treat $$\mathrm{city}$$ (equivalent to the $$\mathrm{in\_sf}$$ variable) as a categorical output with classes $$\mathrm{NY},\mathrm{SF}$$.  The rest of variables (except for $$\mathrm{in\_sf}$$) will be treated as input.

Here we reproduce the scatter plot matrix from *visual intro*.

```{r splom}
trellis.par.set(superpose.symbol = list(pch = 20, alpha = 0.2, col = c(my.col <- c("blue", "green3"), trellis.par.get("superpose.symbol")$col[3:7])))
splom(~ home[3:9], data = home, groups = city, auto.key = TRUE, pscales = 0)
```

Elevation seems like an input variable that is informative for distinguishing NY from SF.  But the empirical distribution of elevation for NY overlaps considerably with SF at lower elevations.  Therefore additional variables like $$\mathrm{price\_per\_sqft}$$ would be useful.

```{r elevation-price-per-sqft}
densityplot(~ elevation | city, data = home, groups = city, plot.points = "rug", layout = c(1, 2), xlim = c(0, 250), col = my.col)
xyplot(elevation ~ price_per_sqft, data = home, groups = city, col = my.col)
```

## CART / decision tree

This figure 9.2 from Hastie et al 2009 explains the recursive partitioning of CART.

![Fig]({{ site.baseurl }}/figures/elements-stats-learning-fig-9.2.jpg)

We first fit the decision tree

```{r complex-tree}
M <- list()
M$complex.tree <- rpart(city ~ beds + bath + price + year_built + sqft + price_per_sqft + elevation, data = home, control = rpart.control(cp = -1))
plot(M$complex.tree, margin = 0.01)
text(M$complex.tree, col = "brown", font = 2, use.n = FALSE, all = FALSE, cex = 0.8)
```

```{r complexity}
plotcp(M$complex.tree, upper = "splits")
```

```{r tree}
M$tree <- rpart(city ~ beds + bath + price + year_built + sqft + price_per_sqft + elevation, data = home)
plot(M$tree, margin = 0.01)
text(M$tree, col = "brown", font = 2, use.n = TRUE, all = TRUE, cex = 0.9)
```

## Classification

Let's predict the city at the average of each input variable taking the following values:

```{r}
(newhome <- data.frame(lapply(home[c(-1, -2)], function(h) round(mean(h)))))
```

Now the prediction.  Actually, it's more than mere prediction of the output classes (NY and SF) because their probability is estimated.  Note that the first splitting rule of the tree already classifies the new data point as SF because $$\mathrm{elevation} = 40 \ge 30.5$$.  Therefore the model based probability estimates are compared to the fraction of those NY and SF homes in the training data whose $$\mathrm{elevation} \ge 30.5$$, which is $$9 / 183$$.  The two kinds of probability estimate are only slightly different: they agree that $$\mathrm{Pr}(\mathrm{city} = \mathrm{SF} \mid \mathrm{elevation} \ge 30.5) \gg \mathrm{Pr}(\mathrm{city} = \mathrm{NY} \mid \mathrm{elevation} \ge 30.5)$$:

```{r barchart-prob}
nh.prob <- predict(M$tree, newhome, type = "prob")
nh.prob <- rbind(nh.prob, data.frame(NY = 9 / 183, SF = (183 - 9) / 183))
nh.prob$prob.estimate <- c("prediction", "training data, elevation >= 30.5")
nh.prob
nh.prob <- reshape(nh.prob, varying = c("NY", "SF"), v.names = "probability", times = c("NY", "SF"), timevar = "city", direction = "long")
barchart(probability ~ prob.estimate, data = nh.prob, groups = city, stack = TRUE, auto.key = TRUE, scales = list(x = list(cex = 1)))
```

The next prediction is at a sequence of increasing $$\mathrm{price\_per\_sqft}$$ in the range of the training data, while holding all other input variables at their average.

```{r}
nh <- cbind(newhome[-6], data.frame(price_per_sqft = seq(from = min(home$price_per_sqft), to = max(home$price_per_sqft), length.out = 50)))
nh.prob <- predict(M$tree, nh, type = "prob")
```

```{r effect-price-per-sqft}
my.xyplot <- function(fm = formula(NY ~ price_per_sqft), newdata = nh, mod = M)
    xyplot(fm, data = cbind(newdata, predict(mod$tree, newdata, type = "prob")),
           ylim = c(-0.05, 1.05), ylab = "Pr(city = NY)",
           panel = function(...) { panel.grid(h = -1, v = -1); panel.xyplot(...) })
my.xyplot()
```

Clearly, the model based probability estimates show no dependence on $$\mathrm{price\_per\_sqft}$$, because the tree ends in a leaf if $$\mathrm{elevation} \ge 30.5$$ (the right branch of the first split).  So let's set $$\mathrm{elevation} = 30.0$$ and repeat the prediction.  This time there is a clear dependence on $$\mathrm{price\_per\_sqft}$$.  It is a striking limitation of the decision tree model that the dependence is step-like.  Note that the step is located at $$\mathrm{price\_per\_sqft} = 741.5$$, which corresponds to the second of the two splits based on $$\mathrm{price\_per\_sqft}$$.  There is a tiny step at $$1072$$ corresponding to the first of the splits based on $$\mathrm{price\_per\_sqft}$$.  This tiny step goes counter-intuitively *downward* exposing another weakness of decision trees.

```{r effect-price-per-sqft-elevation-30}
nh$elevation <- 30
my.xyplot()
```

Finally, let's do the same type of experiment when $$\mathrm{elevation}$$ grows incrementally.  Similarly to the previous case, there is a sudden jump around a splitting threshold, which is in this case $$\mathrm{elevation} = 30.5$$, corresponding to the first (topmost) split in the tree.

```{r effect-elevation}
nh <- cbind(newhome[-7], data.frame(elevation = seq(from = min(home$elevation), to = max(home$elevation), length.out = 50)))
my.xyplot(fm = formula(NY ~ elevation))
```

